{
    "contents" : "---\ntitle: \"Coursera Machine Learning Project\"\nauthor: \"Mark Speicher\"\ndate: \"Sunday, December 21, 2014\"\noutput: html_document\n---\nPractical Machine Learning Project\n------------------------------------------------------------------------------------\n**Problem**\n\n*Note*\nThis documentation provides information and R code for the course project for the Coursera course, \"Practical Machine Learning,\" offered through Johns Hopkins University.\n\n*Background* \n(from the *Coursera* course site here: https://class.coursera.org/predmachlearn-016/human_grading/view/courses/973763/assessments/4/submissions)\n\nUsing devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. \n\nIn this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. \n\nMore information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).\n\nData Sources\n-----------------------------------------------------------------------------------\n\nThe training data for this project were downloaded from:\n\nhttps://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\n\nThe test data were downloaded from:\n\nhttps://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\n\nData source: http://groupware.les.inf.puc-rio.br/har\n\nIntended results\n------------------------------------------------------------------------------------\n*Goals*\nThe goal of the project is to predict the manner in which the subjects did the exercise. This is the “classe” variable in the training set. Any of the other variables may be used to predict with. I then used the prediction model to predict 20 different test cases.\n\n*Submission*\nThe resulting submission should consist of a link to a Github repo with the R markdown and compiled HTML files describing this analysis. Please constrain the text of the writeup to < 2000 words and the number of figures to be less than 5. \n\nAccording to the instructions, \"You should also apply your machine learning algorithm to the 20 test cases available in the test data above. Please submit your predictions in appropriate format to the programming assignment for automated grading. See the programming assignment for additional details.\"\n\nGetting the Data\n------------------------------------------------------------------------------------\nIn the code below, the data is downloaded directly from the URL provided. You will also need to install and load a number of R packages in order to reproduce this analysis from the code provided.\n\n```{r}\n## Begin by loading the required packages\n## If they are not installed in your system then install them from Cran-R \n## using install.packages('package_name')\nlibrary(caret)\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(RColorBrewer)\nlibrary(rattle)\nlibrary(e1071)\nlibrary(randomForest)\n\n## Get the data from the URLs in the assignment and set the NA variables to NA\ntrainUrl <- \"http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\"\ntestUrl <- \"http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\"\ntraining <- read.csv(url(trainUrl), na.strings=c(\"NA\",\"#DIV/0!\",\"\"))\ntesting <- read.csv(url(testUrl), na.strings=c(\"NA\",\"#DIV/0!\",\"\"))\n\n```\n\nPartitioning the Data\n------------------------------------------------------------------------------------\nIt is customary to use the training data for both training the algorithm and testing the method chosen; the code is then applied to the actual testing set. In this case, I used 60% of the original training set for training, and the other 40% for testing.\n\n```{r}\n## Particion the training data set for 60% training 40% testing\ninTrain <- createDataPartition(y=training$classe, p=0.6, list=FALSE)\nmyTraining <- training[inTrain, ]; myTesting <- training[-inTrain, ]\ndim(myTraining); dim(myTesting)\n## You should have 11,776 cases in training and 7,846 in testing\n\n```\n\nCleaning the Data\n------------------------------------------------------------------------------------\nIn the code below, the data is tidied using the methods described.\n\n```{r}\n\n## Clean the data\n\n## Because the file is so big we will look for variables with\n## Near-zero variances\nmyDataNZV <- nearZeroVar(myTraining, saveMetrics=TRUE)\n\n## Take a look\nmyNZVvars <- names(myTraining) %in% c(\"new_window\", \"kurtosis_roll_belt\", \"kurtosis_picth_belt\", \"kurtosis_yaw_belt\", \"skewness_roll_belt\", \"skewness_roll_belt.1\", \"skewness_yaw_belt\", \"max_yaw_belt\", \"min_yaw_belt\", \"amplitude_yaw_belt\", \"avg_roll_arm\", \"stddev_roll_arm\", \"var_roll_arm\", \"avg_pitch_arm\", \"stddev_pitch_arm\", \"var_pitch_arm\", \"avg_yaw_arm\", \"stddev_yaw_arm\", \"var_yaw_arm\", \"kurtosis_roll_arm\", \"kurtosis_picth_arm\", \"kurtosis_yaw_arm\", \"skewness_roll_arm\", \"skewness_pitch_arm\", \"skewness_yaw_arm\",                     \"max_roll_arm\", \"min_roll_arm\", \"min_pitch_arm\", \"amplitude_roll_arm\", \"amplitude_pitch_arm\", \"kurtosis_roll_dumbbell\", \"kurtosis_picth_dumbbell\", \"kurtosis_yaw_dumbbell\", \"skewness_roll_dumbbell\", \"skewness_pitch_dumbbell\", \"skewness_yaw_dumbbell\", \"max_yaw_dumbbell\", \"min_yaw_dumbbell\", \"amplitude_yaw_dumbbell\", \"kurtosis_roll_forearm\", \"kurtosis_picth_forearm\", \"kurtosis_yaw_forearm\", \"skewness_roll_forearm\", \"skewness_pitch_forearm\", \"skewness_yaw_forearm\", \"max_roll_forearm\", \"max_yaw_forearm\", \"min_roll_forearm\", \"min_yaw_forearm\", \"amplitude_roll_forearm\", \"amplitude_yaw_forearm\", \"avg_roll_forearm\", \"stddev_roll_forearm\", \"var_roll_forearm\", \"avg_pitch_forearm\", \"stddev_pitch_forearm\", \"var_pitch_forearm\", \"avg_yaw_forearm\", \"stddev_yaw_forearm\", \"var_yaw_forearm\")\n\nmyTraining <- myTraining[!myNZVvars]\n\n## To check the new number of observations\ndim(myTraining)\n## You should now have 100 variables for each case where before you had 160\n\n## Eliminate the ID numbers as they will mess up the correlations\nmyTraining <- myTraining[c(-1)]\n\n## Get rid of the variables with too many NAs in the set\ntrainingV3 <- myTraining #creating another subset to iterate in loop\nfor(i in 1:length(myTraining)) { #for every column in the training dataset\n  if( sum( is.na( myTraining[, i] ) ) /nrow(myTraining) >= .6 ) { #if n?? NAs > 60% of total observations\n    for(j in 1:length(trainingV3)) {\n      if( length( grep(names(myTraining[i]), names(trainingV3)[j]) ) ==1)  { #if the columns are the same:\n        trainingV3 <- trainingV3[ , -j] #Remove that column\n      }   \n    } \n  }\n}\n\n## To check the new N again\ndim(trainingV3)\n## You should have 58 variables now rather than 60\n\n## We're done cleaning the myTraining training set so let's\n## Set it back to the original name and clean up our extra sets:\nmyTraining <- trainingV3\nrm(trainingV3)\n\n## Clean up the other sets using the same procedure\n## On the MyTraining testing set and the actual testing set\nclean1 <- colnames(myTraining)\nclean2 <- colnames(myTraining[, -58]) # already with classe column removed\nmyTesting <- myTesting[clean1]\ntesting <- testing[clean2]\n\n## Check the new N\ndim(myTesting)\ndim(testing)\n\n## Coerce the data to the same type to work with the trees software\nfor (i in 1:length(testing) ) {\n  for(j in 1:length(myTraining)) {\n    if( length( grep(names(myTraining[i]), names(testing)[j]) ) ==1)  {\n      class(testing[j]) <- class(myTraining[i])\n    }      \n  }      \n}\n\n## And to make sure Coertion really worked, simple smart ass technique:\ntesting <- rbind(myTraining[2, -58] , testing) # remove row 2 as it means nothing anything\ntesting <- testing[-1,]\n\n```\n\nAnalysis\n------------------------------------------------------------------------------------\nTwo methods are described below for the analysis, with a comparison to determine the superior predictive method. We try the decision tree on the training partition of the training set.\n\n```{r}\n## First, use the decision tree algorithm to \n## identify the groupings of teh variables for predictions\nmodFitA1 <- rpart(classe ~ ., data=myTraining, method=\"class\")\n\n## Take a look at the graph\nfancyRpartPlot(modFitA1)\n\n\n```\n\nDecision Tree Plot\n\n```{r, echo=FALSE}\nfancyRpartPlot(modFitA1)\n\n```\nPredictions and Cross-Validation\n------------------------------------------------------------------------------------\n*Decision Tree*\nInitially, a decision-tree model was used for the predictive algorithm on the training set. To check the internal cross-validation, we use the model on the testing particion of the training set. Decision trees are commonly used with large data sets and this analysis gave good results.\n\n```{r}\n## Now do the predictions on myTesting set\npredictionsA1 <- predict(modFitA1, myTesting, type = \"class\")\n\n## Use confusionMatrix to test the results\nconfusionMatrix(predictionsA1, myTesting$classe)\n## Take a look at your results - overall accuracy should be\n## 0.8663 (95% CI 0.08586 - 0.08738)\n\n```\n*Random Forest Model*\nNow we try a random forest model, due to its ability to balance errors in unbalanced datasets, both on the training partition then on the testing partition of the training set.\n\n```{r}\n## Let's compare to another method: Random Forest groupings\nmodFitB1 <- randomForest(classe ~. , data=myTraining)\npredictionsB1 <- predict(modFitB1, myTesting, type = \"class\")\nconfusionMatrix(predictionsB1, myTesting$classe)\n## Wow! Much better - overall accuracy is\n## 0.9985 (95% CI 0.9973, 0.9992)\n\n```\n*Results*\nThe random forest model was superior in accuracy and internal cross-validation. We now use it to predict the testing set.\n\n## So now we work on the actual testing set\npredictionsB2 <- predict(modFitB1, testing, type = \"class\")\n\n## And we do our output files (hoping they are right!)\npml_write_files = function(x){\n  n = length(x)\n  for(i in 1:n){\n    filename = paste0(\"problem_id_\",i,\".txt\")\n    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)\n  }\n}\n\npml_write_files(predictionsB2)\n```\n",
    "created" : 1419200102652.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "885102459",
    "id" : "39C53EFE",
    "lastKnownWriteTime" : 1419203185,
    "path" : "~/Coursera/R/MachineLearning/PracticalMachineLearning_Speicher.Rmd",
    "project_path" : null,
    "properties" : {
        "tempName" : "Untitled1"
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}